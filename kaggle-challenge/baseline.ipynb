{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import os\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences):\n",
    "    # Build vocabulary\n",
    "    word_counts = Counter(itertools.chain(*sentences))\n",
    "    # Mapping from index to word\n",
    "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "    # Mapping from word to index\n",
    "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "    return word_counts, vocabulary, vocabulary_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(inp_data, vocabulary_inv, size_features=100,\n",
    "                   mode='skipgram',\n",
    "                   min_word_count=2,\n",
    "                   context=5):\n",
    "    model_name = \"embedding\"\n",
    "    model_name = os.path.join(model_name)\n",
    "    num_workers = 15  # Number of threads to run in parallel\n",
    "    downsampling = 1e-3  # Downsample setting for frequent words\n",
    "    print('Training Word2Vec model...')\n",
    "    sentences = [[vocabulary_inv[w] for w in s] for s in inp_data]\n",
    "    if mode == 'skipgram':\n",
    "        sg = 1\n",
    "        print('Model: skip-gram')\n",
    "    elif mode == 'cbow':\n",
    "        sg = 0\n",
    "        print('Model: CBOW')\n",
    "    embedding_model = word2vec.Word2Vec(sentences, workers=num_workers,\n",
    "                                        sg=sg,\n",
    "                                        size=size_features,\n",
    "                                        min_count=min_word_count,\n",
    "                                        window=context,\n",
    "                                        sample=downsampling)\n",
    "    embedding_model.init_sims(replace=True)\n",
    "    print(\"Saving Word2Vec model {}\".format(model_name))\n",
    "    embedding_weights = np.zeros((len(vocabulary_inv), size_features))\n",
    "    for i in range(len(vocabulary_inv)):\n",
    "        word = vocabulary_inv[i]\n",
    "        if word in embedding_model:\n",
    "            embedding_weights[i] = embedding_model[word]\n",
    "        else:\n",
    "            embedding_weights[i] = np.random.uniform(-0.25, 0.25,\n",
    "                                                     embedding_model.vector_size)\n",
    "    return embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.add('would')\n",
    "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
    "    preprocessed_sentences = []\n",
    "    for i, row in df.iterrows():\n",
    "        sent = row[\"text\"]\n",
    "        sent_nopuncts = sent.translate(translator)\n",
    "        words_list = sent_nopuncts.strip().split()\n",
    "        filtered_words = [word for word in words_list if word not in stop_words and len(word) != 1]\n",
    "        preprocessed_sentences.append(\" \".join(filtered_words))\n",
    "    df[\"text\"] = preprocessed_sentences\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model...\n",
      "Model: skip-gram\n",
      "Saving Word2Vec model embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mbfdc\\Anaconda3\\envs\\work\\lib\\site-packages\\ipykernel_launcher.py:28: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "C:\\Users\\mbfdc\\Anaconda3\\envs\\work\\lib\\site-packages\\ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "C:\\Users\\mbfdc\\Anaconda3\\envs\\work\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\mbfdc\\Anaconda3\\envs\\work\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "data_path = \"Data/\"\n",
    "\n",
    "df_train = pd.read_csv(data_path + \"train.csv\")\n",
    "df_test = pd.read_csv(data_path + \"test.csv\")\n",
    "\n",
    "df_train[\"text\"] = df_train[\"review\"]\n",
    "df_test[\"text\"] = df_test[\"review\"]\n",
    "df_train = preprocess_df(df_train)\n",
    "\n",
    "df_test = preprocess_df(df_test)\n",
    "\n",
    "tagged_data = [word_tokenize(_d) for i, _d in enumerate(df_train[\"text\"])]\n",
    "word_counts, vocabulary, vocabulary_inv = build_vocab(tagged_data)\n",
    "inp_data = [[vocabulary[word] for word in text] for text in tagged_data]\n",
    "embedding_weights = get_embeddings(inp_data, vocabulary_inv)\n",
    "\n",
    "\n",
    "tagged_train_data = [word_tokenize(_d) for i, _d in enumerate(df_train[\"text\"])]\n",
    "tagged_test_data = [word_tokenize(_d) for i, _d in enumerate(df_test[\"text\"])]\n",
    "\n",
    "train_vec = []\n",
    "for doc in tagged_train_data:\n",
    "    vec = 0\n",
    "    for w in doc:\n",
    "        vec += embedding_weights[vocabulary[w]]\n",
    "    vec = vec / len(doc)\n",
    "    train_vec.append(vec)\n",
    "\n",
    "test_vec = []\n",
    "for doc in tagged_test_data:\n",
    "    vec = 0\n",
    "    length = 0\n",
    "    for w in doc:\n",
    "        try:\n",
    "            vec += embedding_weights[vocabulary[w]]\n",
    "            length += 1\n",
    "        except:\n",
    "            continue\n",
    "    vec = vec / length\n",
    "    test_vec.append(vec)\n",
    "\n",
    "clf = LogisticRegression(max_iter=100000000).fit(train_vec, df_train[\"label\"])\n",
    "preds = clf.predict(test_vec)\n",
    "\n",
    "dic = {\"Id\": [], \"Predicted\": []}\n",
    "for i, pred in enumerate(preds):\n",
    "    dic[\"Id\"].append(i)\n",
    "    dic[\"Predicted\"].append(pred)\n",
    "\n",
    "dic_df = pd.DataFrame.from_dict(dic)\n",
    "dic_df.to_csv(data_path + \"predicted.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05187569,  0.07635441,  0.0185177 ,  0.07069988, -0.01644366,\n",
       "       -0.07307199, -0.14826442, -0.15318319, -0.03870092,  0.01208771,\n",
       "        0.04295781, -0.05468978,  0.01027161, -0.04434469, -0.10359234,\n",
       "       -0.02489309, -0.02115308,  0.05459466, -0.0199364 ,  0.02917993,\n",
       "        0.08272176, -0.04944272,  0.00689092,  0.0334314 , -0.01661962,\n",
       "       -0.02835405,  0.03777434, -0.03238566, -0.03536997,  0.0881428 ,\n",
       "        0.05693689, -0.06150929,  0.12330765, -0.0092267 , -0.09279685,\n",
       "       -0.08054416,  0.115674  ,  0.12240585, -0.03147379, -0.02998215,\n",
       "       -0.11217732,  0.04644492, -0.0036658 ,  0.0528971 ,  0.1306516 ,\n",
       "       -0.08707248,  0.01716317,  0.00494881, -0.01466959,  0.08002137,\n",
       "        0.1077945 , -0.05920809,  0.08259955, -0.01092692, -0.04617465,\n",
       "        0.00140286,  0.06613625, -0.00230198, -0.08665175,  0.01540626,\n",
       "        0.04815543, -0.04479445,  0.09404107,  0.08037522,  0.00599573,\n",
       "        0.05791441, -0.00473196, -0.02989393,  0.14082607, -0.0321821 ,\n",
       "       -0.03911949, -0.03762728,  0.03018751, -0.01670169, -0.07360433,\n",
       "       -0.01886559, -0.02307549,  0.02209385, -0.01677079,  0.07028476,\n",
       "       -0.0088746 ,  0.00264554,  0.06357871, -0.00839262, -0.08527453,\n",
       "        0.10220561, -0.08188379, -0.07486292,  0.03783224,  0.08907747,\n",
       "        0.03349578,  0.009788  , -0.04834884, -0.01944993,  0.01087678,\n",
       "       -0.11335116, -0.0238012 , -0.00331564, -0.03702277,  0.04112614])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vec[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
