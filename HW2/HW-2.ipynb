{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Paper Discussion\n",
    "#### (a).\n",
    "SegPhrase:\n",
    "- Build a candidate phrases set by frequent pattern mining\n",
    "- Extract phrase features based on concordance and informativeness creteria\n",
    "- Train a classifier on limited labeled data to predict a quality score\n",
    "- Partition a sequence of words by maximizing the likelihood and filter out low rectified frequency phrases\n",
    "- Using rectified frequency, re-compute phrase features and re-train the classfier\n",
    "\n",
    "AutoPhrase:\n",
    "- Build a positive pool:mining high-quality phrases from public knowledge bases\n",
    "- Build a noisy negative pool: obtaining candidate phrases from the given corpus but not matching any high-quality phrases in the positive poll\n",
    "- Constructing a good ensemble of base classifiers for predicting phrase quality scores in order to reduce the impact of the false negative phrases\n",
    "- Apply a POS-guided phrasal segmentation algorithm to find the best segmentation for each sentence by utilizing its shallow, language-specific knowledge\n",
    "- Re-compute statistical features based on the recitified frequency of phrases and re-estimate phrase qualities\n",
    "\n",
    "\n",
    "#### (b).\n",
    "SegPhrase is not a competely unsupervised framework. When applying to a new corpus, the high-quality phrases under this new context are subject to change. Therefore, the phrase features extracted in this new corpus are very likely to have a different distribution from the original training corpus, thus requiring a classifier re-training. Human effort is still needed to label some high-quality and low-quality phrases in this new corpus.\n",
    "\n",
    "#### (c).\n",
    "Motivation:\n",
    "- SegPhrase requires a labeled phrase dataset annotated by domain experts, which can be expensive, especially in specialized domains.\n",
    "- The prior linguistic information hasn't been fully utilized in SegPhrase\n",
    "\n",
    "Novelty:\n",
    "- Apply distant supervision to avoid the need of labeled data\n",
    "- Utilize shallow linguistic knowledge to increase phrasal segmentation accuracy\n",
    "\n",
    "#### (d).\n",
    "To save human effort. If polling strategy is not used, then all the phrases generated by every method need to be labeled manually in order to compute precision and recall of each method, which can be very expensive.\n",
    "\n",
    "#### (e).\n",
    "One thing I noticed is that both SegPhrase and AutoPhrase follow a frequent pattern mining manner in the candidate phrase generation stage. The minimum support threshold are fixed to be 30 in these two methods, which makes it competely impossible to extract those high-quality but infrequent phrases. However, it's very likey to have some high-quality phrases that only appear a few times because they are only relevant to some certain niche fields.\n",
    "\n",
    "#### (f).\n",
    "Increase the recall of candidate phrase generation stage by taking in more infrequent but potentially high-quality phrases. Pretrained POS tagger or sequence labeling model (BIOES) may be utilized to help identify these phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Phrase Mining experiments\n",
    "\n",
    "#### (b).\n",
    "- I noticed that for some phrases, if one of their words change from plural form to singular form, their quality scores will change greatly:\n",
    "  - robotic tasks(0.87) => robotic task(0.34)\n",
    "  - machine learning classifiers(0.76) => machine learning classifier(0.41)\n",
    "  - ppm models(0.57) => ppm model(0.38)\n",
    "- Treating the singular form and the plural form of one phrase as two different will definitely underestimate the real quality score of this phrase. I think singularizing plural nouns should be performed before tokenization to solve this problem.\n",
    "<hr>\n",
    "- Some non-English low-quality phrases are assigned with very high scores, like le_problème(score: 0.919, French, means the_problem) and zur_behandlung(score: 0.832, German, means for_treatment). These phrases contain non-English stop words that were not taken into account when estimating quality scores. I think the stop words set should be updated and includes stop words of other languages.\n",
    "<hr>\n",
    "- I also found some phrases with very high quality scores but I can't recognize, such as \"cl sr\" (0.869) and \"vc pm\" (0.869). I'm not sure where these phrases come from and what they stand for, and I also don't understant what is the cause.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c). Apply Word2Vec on segmented corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(iv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from gensim import utils, models\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_punc(text):\n",
    "    punc = '!\"#$%&\\'()*,-./:;<=>?@[\\\\]^`{|}~'\n",
    "    table = str.maketrans(punc, ' '*len(punc))\n",
    "    return text.translate(table)\n",
    "\n",
    "def merge_phrase(text):\n",
    "    start_inds = [m.span() for m in re.finditer('<phrase>', text)]\n",
    "    end_inds = [m.span() for m in re.finditer('</phrase>', text)]\n",
    "\n",
    "    res = ''\n",
    "    for i, start_pos in enumerate(start_inds):\n",
    "        end_pos = end_inds[i]\n",
    "        if i == 0:\n",
    "            res += text[:start_pos[0]]\n",
    "        \n",
    "        tmp_phrase = text[start_pos[1]: end_pos[0]]\n",
    "        phrase = '_'.join(tmp_phrase.split())\n",
    "        res += phrase\n",
    "        if i == len(start_inds) - 1:\n",
    "            res += text[end_pos[1]:]\n",
    "        else:\n",
    "            res += text[end_pos[1]: start_inds[i+1][0]]\n",
    "    return remove_punc(res).strip().lower()\n",
    "\n",
    "with open('segmentation.txt', 'r') as f:\n",
    "    segs = f.readlines()\n",
    "\n",
    "processed = list()\n",
    "for text in segs:\n",
    "    text = text.strip()\n",
    "    if text == '.':\n",
    "        continue\n",
    "    processed_text = merge_phrase(text)\n",
    "    processed.append(processed_text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dblp_model = models.Word2Vec(processed, size=100, window=5, min_count=3, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d). Run KMeans on quality phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('AutoPhrase_multi-words.txt') as f:\n",
    "    multi_words = f.readlines()\n",
    "\n",
    "phrase_vec = list()\n",
    "phr2ind = dict()\n",
    "ind2phr = list()\n",
    "for line in multi_words:\n",
    "    score = float(line.strip().split()[0])\n",
    "    phrase = '_'.join(line.strip().split()[1:])\n",
    "    # filter out low-quality phrases\n",
    "    if score < 0.5:\n",
    "        continue\n",
    "    if phrase in dblp_model.wv:\n",
    "        phrase_vec.append(dblp_model.wv[phrase])\n",
    "        ind2phr.append(phrase)\n",
    "\n",
    "for ind, phrase in enumerate(ind2phr):\n",
    "    phr2ind[phrase] = ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase_vec = np.array(phrase_vec)\n",
    "kmeans = KMeans(n_clusters=6, random_state=0).fit(phrase_vec)\n",
    "phrase_distance = kmeans.transform(phrase_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the closest 20 phrases in each cluster\n",
    "def get_closest_phrases(phrase_distance, ind2phr, center):\n",
    "    sort_inds = np.argsort(phrase_distance[:, center])\n",
    "    res = list()\n",
    "    for ind in sort_inds[:20]:\n",
    "        res.append(ind2phr[ind])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = list()\n",
    "for i in range(6):\n",
    "    res.append(get_closest_phrases(phrase_distance, ind2phr, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An explanation of the result\n",
    "I found one of the clusters very interesting (first column below). It contains lots of non-English phrases, like système_pour (French, means system_for), un_proceso(Spanish, means a_process), von_modellen(German, means of_models) and so on. They are actually not high-quality phrases and don't belong to one single topic. I think this is because the non-English documents in the DBLP dataset only account for a small proportion, so our Word2Vec model cannot learn a good representation of these non-English phrases based on their context. However, our model can somehow realize that these phrases are not in English thus making their vector representations very close to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+------------------------------------------+---------------------------------+\n",
      "|   Other Languages    | Image Processing and Pattern Recognition |        Computer Networks        |\n",
      "+----------------------+------------------------------------------+---------------------------------+\n",
      "|       der_emv        |          texture_representation          |     mobile_cellular_networks    |\n",
      "|     système_pour     |            spectral_unmixing             |   dynamic_resource_management   |\n",
      "|      un_proceso      |            wavelet_denoising             | optical_burst_switched_networks |\n",
      "|     le_problème      |             image_clustering             |    cellular_wireless_networks   |\n",
      "|     von_modellen     |        color_and_texture_features        |    optical_transport_networks   |\n",
      "|    zur_behandlung    |                kernel_pca                |       capacity_enhancement      |\n",
      "|     und_aufgaben     |           texture_recognition            |          soft_handover          |\n",
      "|    el_desarrollo     |         image_feature_extraction         |      reliable_multicasting      |\n",
      "|  procesos_software   |           feature_combination            |      mobile_adhoc_networks      |\n",
      "|  para_la_evaluación  |           local_image_features           | personal_communication_networks |\n",
      "|    die_bewertung     |           laplacian_eigenmaps            |       topology_management       |\n",
      "|       un_sgbd        |            feature_generation            |    reliable_data_transmission   |\n",
      "|    zur_konzeption    |           texture_descriptors            |    adaptive_admission_control   |\n",
      "|  nouvelle_approche   |            speckle_reduction             |          ring_networks          |\n",
      "|    des_ensembles     |             local_descriptor             |        handoff_management       |\n",
      "| künstliche_neuronale |           features_extraction            |       distributed_caching       |\n",
      "|      suche_nach      |           feature_descriptors            |        packet_aggregation       |\n",
      "|      daten_und       |          feature_distributions           |       handover_management       |\n",
      "|    la_evaluación     |        facial_feature_extraction         |     cellular_mobile_networks    |\n",
      "|     bericht_aus      |           foreground_detection           |       connection_oriented       |\n",
      "+----------------------+------------------------------------------+---------------------------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "table = PrettyTable()\n",
    "table.add_column('Other Languages', res[0])\n",
    "table.add_column('Image Processing and Pattern Recognition', res[1])\n",
    "table.add_column('Computer Networks', res[2])\n",
    "print (table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+-----------------------+---------------------------------------------+\n",
      "| Computer-supported Collaborative Learning |   Valuation Metrics   |                 Mathematics                 |\n",
      "+-------------------------------------------+-----------------------+---------------------------------------------+\n",
      "|            collaboration_tools            |    throughput_gain    |              rayleigh_quotient              |\n",
      "|              service_science              |     search_effort     |              circulant_matrices             |\n",
      "|           collaboration_support           |      query_delay      |               type_inequality               |\n",
      "|       healthcare_information_systems      |     delivery_rate     |           proximal_point_algorithm          |\n",
      "|            learning_technology            |  aggregate_throughput |            equivalence_relations            |\n",
      "|           collaboration_systems           |   bandwidth_overhead  |             hermite_polynomials             |\n",
      "|          educational_applications         |    packet_overhead    |                groebner_bases               |\n",
      "|          technical_communication          |     packet_latency    |               matrix_products               |\n",
      "|       executive_information_systems       |   buffer_utilization  |            semidefinite_programs            |\n",
      "|           software_reengineering          |        hit_rate       |              numeration_systems             |\n",
      "|              semantic_desktop             |   computation_costs   |             characteristic_sets             |\n",
      "|        clinical_information_systems       |     power_overhead    |            binomial_coefficients            |\n",
      "|           engineering_processes           |       fault_rate      |            fractional_programming           |\n",
      "|            end_user_development           |     result_quality    |              polynomial_ideals              |\n",
      "|           educational_multimedia          |     blocking_rate     |             graph_homomorphisms             |\n",
      "|        medical_information_systems        |  average_packet_delay |               wreath_products               |\n",
      "|         collaborative_technologies        |       miss_ratio      |            closest_vector_problem           |\n",
      "|          technical_infrastructure         |  scheduling_overhead  | quantified_constraint_satisfaction_problems |\n",
      "|         collaborative_engineering         |     routing_delay     |                 chain_graphs                |\n",
      "|               virtual_campus              | collision_probability |      nonlinear_complementarity_problem      |\n",
      "+-------------------------------------------+-----------------------+---------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "table = PrettyTable()\n",
    "table.add_column('Computer-supported Collaborative Learning', res[3])\n",
    "table.add_column('Valuation Metrics', res[4])\n",
    "table.add_column('Mathematics', res[5])\n",
    "print (table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
